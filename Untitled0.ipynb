{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM/6XPCGFDCOw6v5UBY/gVK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["! pip install torchshow"],"metadata":{"id":"aE4t7ToVrEJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YiLETilqv1J"},"outputs":[],"source":["import zipfile\n","import os\n","import torch\n","import tifffile as tiff\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchshow as ts\n","import torchvision.transforms.functional as F\n","import time\n","import random\n","\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import Adam\n","from torchsummary import summary\n","from torch.nn import BCEWithLogitsLoss\n","\n","print(torch.__version__)"]},{"cell_type":"code","source":["# see what GPU is currently being used\n","\n","print(torch.cuda.device_count())\n","print(torch.cuda.current_device())\n","print(torch.cuda.get_device_name())"],"metadata":{"id":"AN67NgP5q53J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n"],"metadata":{"id":"XeFyhyfErCD2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PREPARE DATASET"],"metadata":{"id":"SxLC68HWrSC1"}},{"cell_type":"markdown","source":["The dataset consists of 19,200 images of cells but only 1,200 masks. The first step is to match up the images with their ground truth masks.\n"],"metadata":{"id":"SifHJTbOrZ8d"}},{"cell_type":"code","source":["\n","!wget --no-check-certificate \\\n","    \"https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_images.zip\" \\\n","    -O \"/tmp/BBBC005_v1_images.zip\"\n","\n","\n","zip_ref = zipfile.ZipFile('/tmp/BBBC005_v1_images.zip', 'r') #Opens the zip file in read mode\n","zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n","zip_ref.close()"],"metadata":{"id":"1SElognJwlF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget --no-check-certificate \\\n","    \"https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_ground_truth.zip\" \\\n","    -O \"/tmp/BBBC005_v1_ground_truth.zip\"\n","\n","\n","zip_ref = zipfile.ZipFile('/tmp/BBBC005_v1_ground_truth.zip', 'r') #Opens the zip file in read mode\n","zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n","zip_ref.close()"],"metadata":{"id":"crop8xy60EPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = '/tmp/BBBC005_v1_images/'\n","mask_path = '/tmp/BBBC005_v1_ground_truth/'\n","\n","image_paths = sorted(os.listdir(image_path))\n","mask_paths = sorted(os.listdir(mask_path))\n","\n","len(image_paths), len(mask_paths)"],"metadata":{"id":"kzYOjG8YrbEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_paths[:5], mask_paths[:5]"],"metadata":{"id":"OPmTpRCArmOo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We want to remove the '.htaccess' file because it is of no use to us."],"metadata":{"id":"m_D_uUI6sHsY"}},{"cell_type":"code","source":["image_paths.remove('.htaccess')\n","mask_paths.remove('.htaccess')\n","\n","len(image_paths), len(mask_paths)"],"metadata":{"id":"o93d1TVwsF5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_paths[:5], mask_paths[:5]"],"metadata":{"id":"qTXDckJTsF2V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we want to create a list of all images that have an associated mask:"],"metadata":{"id":"7ubyb2htsNzc"}},{"cell_type":"code","source":["new_list = sorted(list(set(image_paths) & set(mask_paths)))\n","len(new_list), type(new_list)"],"metadata":{"id":"Atu4izGCsFzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_list[:5]"],"metadata":{"id":"N_zuzwQIsFwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have our list of 1200 images and their associated masks, lets split this into train and test sets:"],"metadata":{"id":"PRdBljrcsSq4"}},{"cell_type":"code","source":["train_list, test_list = train_test_split(new_list, test_size=0.1)\n","\n","len(train_list), len(test_list)"],"metadata":{"id":"3_gB-SAWsFt9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now lets visualize a couple of images and their ground truth segmentations:"],"metadata":{"id":"rqAZSbuQsZ7F"}},{"cell_type":"code","source":["img = tiff.imread(image_path + train_list[150])\n","mask = tiff.imread(mask_path + train_list[150])\n","\n","img.shape, mask.shape"],"metadata":{"id":"pEDnrfxZsFq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(15,15))\n","\n","fig.add_subplot(1,2,1)\n","plt.title('image')\n","plt.imshow(img, cmap='gray')\n","\n","fig.add_subplot(1,2,2)\n","plt.title('ground truth mask')\n","plt.imshow(mask, cmap='gray')\n","plt.show()"],"metadata":{"id":"HN2h5jiZsFn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"89RgFJQBMJzM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we can see a good example of an image and the ground truth mask. This is what we will be using to train our image segmentation model. The next step is to create a dataset and a dataloader."],"metadata":{"id":"DrKnmmrGsfJd"}},{"cell_type":"code","source":["class SegmentationDataset(Dataset):\n","    def __init__(self, imagePaths, maskPaths, transforms):\n","        # store the image and mask filepaths, and augmentation\n","        self.imagePaths = imagePaths\n","        self.maskPaths = maskPaths\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        # return the number of total samples contained in the dataset\n","        return len(self.imagePaths)\n","    \n","    def __getitem__(self, i):\n","        # grab the image path from the current index\n","        imagePath = self.imagePaths[i]\n","        maskPath = self.maskPaths[i]\n","\n","        image = tiff.imread(image_path + imagePath)\n","        mask = tiff.imread(mask_path + maskPath)\n","        \n","        if self.transforms is not None:\n","            # apply the transformations to both image and its mask\n","            image = self.transforms(image)\n","            mask = self.transforms(mask)\n","        \n","        return image, mask"],"metadata":{"id":"zXikj3iksFlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_batch_size = 64\n","test_batch_size = 64\n","img_size = 128\n","\n","transforms = transforms.Compose([transforms.ToPILImage(),\n","                                 transforms.Resize((img_size, img_size)),\n","                                 transforms.ToTensor()])"],"metadata":{"id":"O-9M2a2esFio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## TRAIN DATASET\n","train_dataset = SegmentationDataset(train_list, train_list, transforms)\n","\n","train_loader = DataLoader(train_dataset, \n","                          batch_size = train_batch_size, \n","                          shuffle = True)\n","\n","## TEST DATASET\n","test_dataset = SegmentationDataset(test_list, test_list, transforms)\n","\n","test_loader = DataLoader(test_dataset, \n","                         batch_size = test_batch_size, \n","                         shuffle = True)"],"metadata":{"id":"rIaKbMiAsFfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainImg, trainLabels = next(iter(train_loader))\n","print('trainImg shape: ', trainImg.shape)\n","print('trainLabels shape: ', trainLabels.shape)\n","\n","print(\"\\n\")\n","print('----------------------------------------------------')\n","\n","testImg, testLabels = next(iter(test_loader))\n","print('testImg shape: ', testImg.shape)\n","print('testLabels shape: ', testLabels.shape)\n","\n"],"metadata":{"id":"ym-cf4Q1sFcy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_batch(image, label, batch_num):\n","    image = image[batch_num,:,:,:].numpy()[0,:,:]\n","    label = label[batch_num,:,:,:].numpy()[0,:,:]\n","\n","    figure, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 7))\n","    \n","    # plot the original image, its mask, and the predicted mask\n","    ax[0].imshow(image, cmap='gray')\n","    ax[1].imshow(label, cmap='gray')\n","    \n","    # set the titles of the subplots\n","    ax[0].set_title(f\"Image # {batch_num}\")\n","    ax[1].set_title(f\"Original Mask # {batch_num}\")\n","    \n","    # set the layout of the figure and display it\n","    figure.tight_layout()\n","    figure.show()\n"],"metadata":{"id":"t32JXz9OsFZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(3):\n","    display_batch(trainImg, trainLabels, i)"],"metadata":{"id":"tJoBSRbQW3AN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we can see a good example of an image and the ground truth mask. This is what we will be using to train our image segmentation model. The next step is to create a dataset and a dataloader.\n"],"metadata":{"id":"qOJdM_dssqWr"}},{"cell_type":"markdown","source":["### Define UNet model"],"metadata":{"id":"ANUVhMk_srQl"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class double_conv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(double_conv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class inconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(inconv, self).__init__()\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class down(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(down, self).__init__()\n","        self.mpconv = nn.Sequential(nn.MaxPool2d(2),\n","                                    double_conv(in_ch, out_ch))\n","\n","    def forward(self, x):\n","        x = self.mpconv(x)\n","        return x\n","\n","\n","class up(nn.Module):\n","    def __init__(self, in_ch, out_ch, bilinear=True):\n","        super(up, self).__init__()\n","\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\",\n","                                  align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2,\n","                                         2, stride=2)\n","\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n","        \n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class outconv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(outconv, self).__init__()\n","        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","        self.inc = inconv(n_channels, 64)\n","        self.down1 = down(64, 128)\n","        self.down2 = down(128, 256)\n","        self.down3 = down(256, 512)\n","        self.down4 = down(512, 512)\n","        self.up1 = up(1024, 256, False)\n","        self.up2 = up(512, 128, False)\n","        self.up3 = up(256, 64, False)\n","        self.up4 = up(128, 64, False)\n","        self.outc = outconv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        x = self.outc(x)\n","        return torch.sigmoid(x)"],"metadata":{"id":"4FC4gGu-sFW8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = UNet(n_channels=1, n_classes=1).float()\n","model.to(device)\n","\n","summary(model, (1, img_size, img_size))"],"metadata":{"id":"3x6ypijFsFUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n","    \"\"\"\n","    Args:\n","        pr (torch.Tensor): A list of predicted elements\n","        gt (torch.Tensor):  A list of elements that are to be predicted\n","        eps (float): epsilon to avoid zero division\n","        threshold: threshold for outputs binarization\n","    Returns:\n","        float: IoU (Jaccard) score\n","    \"\"\"\n","\n","    if activation is None or activation == \"none\":\n","        activation_fn = lambda x: x\n","    elif activation == \"sigmoid\":\n","        activation_fn = torch.nn.Sigmoid()\n","    elif activation == \"softmax2d\":\n","        activation_fn = torch.nn.Softmax2d()\n","    else:\n","        raise NotImplementedError(\n","            \"Activation implemented for sigmoid and softmax2d\"\n","        )\n","\n","    pr = activation_fn(pr)\n","\n","    if threshold is not None:\n","        pr = (pr > threshold).float()\n","\n","\n","    tp = torch.sum(gt * pr)\n","    fp = torch.sum(pr) - tp\n","    fn = torch.sum(gt) - tp\n","\n","    score = ((1 + beta ** 2) * tp + eps) \\\n","            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n","\n","    return score\n","\n","\n","class DiceLoss(nn.Module):\n","    __name__ = 'dice_loss'\n","\n","    def __init__(self, eps=1e-7, activation='sigmoid'):\n","        super().__init__()\n","        self.activation = activation\n","        self.eps = eps\n","\n","    def forward(self, y_pr, y_gt):\n","        return 1 - f_score(y_pr, y_gt, beta=1., \n","                           eps=self.eps, threshold=None, \n","                           activation=self.activation)\n","\n","\n","class BCEDiceLoss(DiceLoss):\n","    __name__ = 'bce_dice_loss'\n","\n","    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):\n","        super().__init__(eps, activation)\n","        if activation == None:\n","            self.bce = nn.BCELoss(reduction='mean')\n","        else:\n","            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n","        self.lambda_dice=lambda_dice\n","        self.lambda_bce=lambda_bce\n","\n","    def forward(self, y_pr, y_gt):\n","        dice = super().forward(y_pr, y_gt)\n","        bce = self.bce(y_pr, y_gt)\n","        return (self.lambda_dice*dice) + (self.lambda_bce* bce)\n","    \n","\n","def dice(img1, img2):\n","    img1 = np.asarray(img1).astype(np.bool)\n","    img2 = np.asarray(img2).astype(np.bool)\n","\n","    intersection = np.logical_and(img1, img2)\n","\n","    return 2.0 * intersection.sum() / (img1.sum() + img2.sum())\n","\n","\n","def dice_no_threshold(\n","    outputs: torch.Tensor,\n","    targets: torch.Tensor,\n","    eps: float = 1e-7,\n","    threshold: float = None,\n","):\n","    \"\"\"\n","    Reference:\n","    https://catalyst-team.github.io/catalyst/_modules/catalyst/dl/utils/criterion/dice.html\n","    \"\"\"\n","    if threshold is not None:\n","        outputs = (outputs > threshold).float()\n","\n","    intersection = torch.sum(targets * outputs)\n","    union = torch.sum(targets) + torch.sum(outputs)\n","    dice = 2 * intersection / (union + eps)\n","\n","    return dice"],"metadata":{"id":"vL8JZBIts54G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = BCEDiceLoss(eps=1.0, activation=None)\n","optimizer = Adam(model.parameters(), lr = 0.005)\n","current_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2,\n","                                                       patience=2, cooldown=2)"],"metadata":{"id":"OxPUKwtAs506"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of epochs to train the model\n","n_epochs = 4\n","train_loss_list = []\n","valid_loss_list = []\n","train_dice_list = []\n","val_dice_list = []\n","lr_rate_list = []\n","\n","valid_loss_min = np.Inf # track change in validation loss\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    train_dice_score = 0.0\n","    val_dice_score = 0.0\n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    bar = tqdm(train_loader, postfix={\"train_loss\":0.0})\n","    for data, target in bar:\n","        # move tensors to GPU\n","        data, target = data.cuda(), target.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        #print(loss)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        \n","        train_dice_cof = dice_no_threshold(output.cpu(), target.cpu()).item() ## train dice cof\n","        train_dice_score +=  train_dice_cof * data.size(0)\n","            \n","        bar.set_postfix(ordered_dict={\"train_loss\":loss.item()})\n","    ######################    \n","    # validate the model #\n","    ######################\n","    model.eval()\n","    del data, target\n","    with torch.no_grad():\n","        bar = tqdm(test_loader, postfix={\"test_loss\":0.0, \"dice_score\":0.0})\n","        for data, target in bar:\n","            # move tensors to GPU\n","            data, target = data.cuda(), target.cuda()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(data)\n","            # calculate the batch loss\n","            loss = criterion(output, target)\n","            # update average validation loss \n","            valid_loss += loss.item()*data.size(0)\n","            \n","            val_dice_cof = dice_no_threshold(output.cpu(), target.cpu()).item()\n","            val_dice_score +=  val_dice_cof * data.size(0)\n","            \n","            bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"dice_score\":val_dice_cof})\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_loader.dataset)\n","    valid_loss = valid_loss/len(test_loader.dataset)\n","    train_dice_score = train_dice_score/len(train_loader.dataset)\n","    val_dice_score = val_dice_score/len(test_loader.dataset)\n","    train_loss_list.append(train_loss)\n","    valid_loss_list.append(valid_loss)\n","    train_dice_list.append(train_dice_score)\n","    val_dice_list.append(val_dice_score)\n","    lr_rate_list.append([param_group['lr'] for param_group in optimizer.param_groups])\n","    \n","    # print training/validation statistics \n","    print('Epoch: {}  Training Loss: {:.6f}  Validation Loss: {:.6f} Traning Dice Score: {:.6f} Valid Dice Score: {:.6f}'.format(\n","        epoch, train_loss, valid_loss, train_dice_score, val_dice_score))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(model.state_dict(), 'model_capstone.pt')\n","        valid_loss_min = valid_loss\n","    \n","    scheduler.step(valid_loss)"],"metadata":{"id":"i5Xw21mos5xq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n","        \n","# plot the original image, its mask, and the predicted mask\n","ax[0].plot(lr_rate_list, marker='o', label=\"learning rate\")\n","ax[1].plot(train_loss_list,  marker='o', label=\"Training Loss\")\n","ax[1].plot(valid_loss_list,  marker='o', label=\"Validation Loss\")\n","ax[2].plot(train_dice_list, marker='o', label=\"Training Dice\")\n","ax[2].plot(val_dice_list, marker='o', label=\"Validation Dice\")\n","# ax[3].plot(time_list, marker='o', label=\"Training Time\")\n","    \n","# set the titles of the subplots\n","ax[0].set_title(\"Learning rate during training\")\n","ax[1].set_title(\"Loss during training\")\n","ax[2].set_title(\"Dice during training\")\n","# ax[3].set_title(\"Training Time\")\n","\n","# add legend for loss an dice\n","ax[1].legend(loc='right')\n","ax[2].legend(loc='right')\n","\n","# set the axis labels of the subplots\n","for i in range(3):\n","    ax[i].set_xlabel('epoch')\n","    ax[0].set_ylabel('learning rate')\n","    ax[1].set_ylabel('loss')\n","    ax[2].set_ylabel('dice')\n","# ax[3].set_ylabel('time (s)')\n","    \n","# set the layout of the figure and display it\n","figure.tight_layout()\n","figure.show()   "],"metadata":{"id":"UZ72ttxvhpzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load best model\n","model.load_state_dict(torch.load('model_capstone.pt'))\n","model.eval();"],"metadata":{"id":"ExtCf8lds5j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = model(testImg.to(device)).cpu().detach()\n","outputs.shape"],"metadata":{"id":"d6ponMQ_h9U7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_pred(image, label, pred, batch_num):\n","    dice_score = dice(pred[batch_num,:,:], label[batch_num,:,:]).item()\n","    image = image[batch_num,:,:,:].numpy()[0,:,:]\n","    label = label[batch_num,:,:,:].numpy()[0,:,:]\n","    pred = pred[batch_num,:,:,:].cpu().detach().numpy()[0,:,:]\n","    \n","    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n","        \n","    # plot the original image, its mask, and the predicted mask\n","    ax[0].imshow(image, cmap='gray')\n","    ax[1].imshow(label, cmap='gray')\n","    ax[2].imshow(pred, cmap='gray')\n","    \n","    # set the titles of the subplots\n","    ax[0].set_title(\"Original Image\")\n","    ax[1].set_title(\"Ground Truth Mask\")\n","    ax[2].set_title(\"Predicted Mask (dice score: {:.4f})\".format(dice_score))\n","    \n","    for i in range(3):\n","        ax[i].set_xticks([])\n","        ax[i].set_yticks([])\n","    \n","    # set the layout of the figure and display it\n","    figure.tight_layout()\n","    figure.show() "],"metadata":{"id":"LThD3XsciBBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in random.sample(range(1, testImg.shape[0]), 3):\n","    display_pred(testImg, testLabels, outputs, i)"],"metadata":{"id":"txVeSJ9KiINl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X5CiUpBetVo-"},"execution_count":null,"outputs":[]}]}